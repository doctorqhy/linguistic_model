#循环神经网络单步执行的时候需要初始化状态，在使用dynamic_run的时候不需要初始化状态
#循环神经网络不同于前馈神经网络之处在于循环神经网络当前时刻的输出与上一时刻的隐藏状态有关


import tensorflow as tf
import numpy as np
from linguistic_model import batch_data


TRAIN_DATA = 'ptb.train'
EVAL_DATA = 'ptb.valid'
TEST_DATA = 'ptb.test'
#隐藏层大小
HIDDEN_SIZE = 300
#LSTM层数
NUM_LAYERS = 2
#词典规模
VOCAB_SIZE = 10000
#训练数据batch大小
TRAIN_BATCH_SIZE = 20
#训练数据截断长度
TRAIN_NUM_STEP = 35

#测试数据batch大小
EVAL_BATCH_SIZE = 1
#测试数据截断长度
EVAL_NUM_STEP = 1
#迭代轮数
NUM_EPOCH = 5
#LSTM节点不被dropout概率
LSTM_KEEP_PROB = 0.9
#词向量不被dropout概率
EMBEDDING_KEEP_PROB = 0.9
MAX_GRAD_NORM = 5
#共享参数
SHARE_EMB_AND_SOFTMAX = True


class PTBModel(object):

    def __init__(self,is_training,batch_size,num_step):

        self.batch_size = batch_size
        self.num_step = num_step

        #定义每一步输入数据和输出的数据
        self.input_data = tf.placeholder(tf.int32,[batch_size,num_step])
        #每个时刻都有输出，因此每个数据的label有35个
        self.target = tf.placeholder(tf.int32,[batch_size,num_step])

        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0
        #定义LSTM结构
        lest_cells = [
            tf.nn.rnn_cell.DropoutWrapper(
                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),
                output_keep_prob=dropout_keep_prob)
            for _ in range(NUM_LAYERS)
        ]
        cell = tf.nn.rnn_cell.MultiRNNCell(lest_cells)

        #初始化隐藏层状态
        self.initial_state = cell.zero_state(batch_size,tf.float32)

        #定义词向量层
        embedding = tf.get_variable('embedding',[VOCAB_SIZE,HIDDEN_SIZE])
        #将输入的数据转化为词向量,词向量格式为[batch,num_step,HIDDEN_SIZE]
        inputs = tf.nn.embedding_lookup(embedding,self.input_data)

        if is_training:

            inputs = tf.nn.dropout(inputs,EMBEDDING_KEEP_PROB)

        outputs = []
        state = self.initial_state

        with tf.variable_scope('RNN'):

            for time_step in range(num_step):

                #从第二步开始，使用前一步的state
                if time_step > 0:

                    tf.get_variable_scope().reuse_variables()

                #得到一个batch数据在每一个时刻的输出，输出格式为[batch,num_step,HIDDEN_SIZE]
                cell_output,state = cell(inputs[:,time_step,:],state)
                outputs.append(cell_output)

        #将output转化为[batch_size*num_step,HIDDEN_SIZE]的形式
        output = tf.reshape(tf.concat(outputs,1),[-1,HIDDEN_SIZE])


      #共享权值
        if SHARE_EMB_AND_SOFTMAX:

            weight = tf.transpose(embedding)

        else:

            weight = tf.get_variable('weight',[HIDDEN_SIZE,VOCAB_SIZE])

        bias = tf.get_variable('bias',[VOCAB_SIZE])
        logits = tf.matmul(output,weight) + bias

        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(self.target,[-1]),logits=logits)
        self.cost = tf.reduce_sum(loss) / batch_size
        self.final_state = state

        if not is_training:return

        #获取可训练的向量
        trainable_variables = tf.trainable_variables()
        #返回缩放好的梯度向量
        grads,_ = tf.clip_by_global_norm(
            tf.gradients(self.cost,trainable_variables),MAX_GRAD_NORM)
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)
        #更新梯度
        self.train_op = optimizer.apply_gradients(zip(grads,trainable_variables))


def run_epoch(session,model,batch,train_op,output_log,step):

    total_cost = 0.0
    iters = 0
    state = session.run(model.initial_state)
    for x,y in batch:

        cost,state,_ = session.run([model.cost,model.final_state,train_op],feed_dict={model.input_data:x,model.target:y,model.final_state:state})

        total_cost += cost
        iters += model.num_step

        if output_log and step%100 == 0:

            print('After %d steps,perplexity is %.3f'%(step,np.exp(total_cost/iters)))
        step += 1

    return step,np.exp(total_cost/iters)


def main():
    initializer = tf.random_uniform_initializer(-0.05,0.05)

    #reuse为None,该上下文管理器创建新变量
    with tf.variable_scope('language_model',reuse=None,initializer=initializer):

        train_model = PTBModel(True,TRAIN_BATCH_SIZE,TRAIN_NUM_STEP)
    #reuse为true,该上下文管理器只能获取已经创建的变量
    with tf.variable_scope('language_model', reuse=True, initializer=initializer):
        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)

    with tf.Session() as sess:

        tf.global_variables_initializer().run()
        #得到训练测试数据
        train_batch = batch_data.make_batches(batch_data.read_data(TRAIN_DATA),TRAIN_BATCH_SIZE,TRAIN_NUM_STEP)
        eval_batch = batch_data.make_batches(batch_data.read_data(EVAL_DATA),EVAL_BATCH_SIZE,EVAL_NUM_STEP)
        test_batch = batch_data.make_batches(batch_data.read_data(TEST_DATA),EVAL_BATCH_SIZE,EVAL_NUM_STEP)

        step = 0

        for i in range(NUM_EPOCH):

            print('In iteration:%d'%(i+1))
            step,train_pplx = run_epoch(sess,train_model,train_batch,train_model.train_op,True,step)
            print('Epoch:%d Train Perplexity:%.3f'%(i+1,train_pplx))
            _, eval_pplx = run_epoch(sess, eval_model, eval_batch, tf.no_op(), False, 0)
            print('Epoch:%d Eval Perplexity:%.3f' % (i + 1, eval_pplx))

        _, test_pplx = run_epoch(sess, eval_model, test_batch, tf.no_op(), False, 0)
        print('Test Perplexity:%.3f' % (test_pplx))


if __name__ == '__main__':
    main()
